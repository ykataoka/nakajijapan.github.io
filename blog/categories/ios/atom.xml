<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[categories: iOS | Nakajijapan]]></title>
  <link href="http://nakajijapan.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://nakajijapan.github.io/"/>
  <updated>2014-04-29T11:42:16+09:00</updated>
  <id>http://nakajijapan.github.io/</id>
  <author>
    <name><![CDATA[nakajijapan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[メディアキャプチャの出力先の実装について]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/04/07/about-avcaptureinput/"/>
    <updated>2014-04-07T02:10:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/04/07/about-avcaptureinput</id>
    <content type="html"><![CDATA[<p><code>AV Foundation</code>を利用して写真の撮影やはたまた動画の撮影を行う場合は、カメラデバイスに接続して映像をリアルタイムに表示させたい場合があるかと思います。</p>

<p><img src="/images/posts/2014-04-06-01.png" alt="AV Foundation" /></p>

<p>そんなときは<code>AVCaptureSession</code>を利用してメディアキャプチャの実装の準備をを行い、入力先・出力先はどうするかという設定を行うのですが、今回はその出力部分について何パターンか方法があったので調べてみました。</p>

<h3>Video Output</h3>

<ul>
<li>AVCaptureMovieFileOutput

<ul>
<li>ムービーファイルを出力するときに利用する</li>
</ul>
</li>
<li>AVCaptureVideoDataOutput

<ul>
<li>キャプチャ中のビデオフレームを処理するときに利用する</li>
</ul>
</li>
<li>AVCaptureStillImageOutput

<ul>
<li>付随するメタデータを使用して静止画像をキャプチャするときに利用する</li>
</ul>
</li>
</ul>


<h3>Audio Output</h3>

<ul>
<li>AVCaptureAudioDataOutput

<ul>
<li>キャプチャ中のオーディオフレームを処理するときに利用する</li>
</ul>
</li>
</ul>


<h2>使い方</h2>

<h3>AVCaptureMovieFileOutput</h3>

<p>ムービーファイルを保存するのに必要な処理を提供してくれるクラス。</p>

<p>録画するときは</p>

<p><code>obj-c
//録画開始
[captureMovieFileOutput startRecordingToOutputFileURL:outputURL recordingDelegate:self];
</code></p>

<p>のように始めることができます。</p>

<p>必要に応じて<code>AVCaptureFileOutputRecordingDelegate</code>オプションを利用して、各タイミングに応じた処理を行います。</p>

<ul>
<li>AVCaptureFileOutputRecordingDelegate</li>
</ul>


<p>```
Delegate Methods</p>

<p>– captureOutput:didStartRecordingToOutputFileAtURL:fromConnections:
– captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:
```</p>

<p>前者は録画を開始したときに、後者は終了したときに呼び出されるメソッドです。例えば、<code>captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:</code>の時に録画したムービーファイルをデバイスに保存する等の処理が行えることができます。</p>

<h4>AVCaptureStillImageOutput</h4>

<p>画像をキャプチャするときに利用して、出力は以下のように実装します。</p>

<p>```
AVCaptureConnection *connection = [[captureStillImageOutput connections] lastObject];
[captureStillImageOutput captureStillImageAsynchronouslyFromConnection:connection</p>

<pre><code>                                                 completionHandler:^(CMSampleBufferRef imageDataSampleBuffer, NSError *error) {
                                                     NSData *data = [AVCaptureStillImageOutput jpegStillImageNSDataRepresentation:imageDataSampleBuffer];
                                                     UIImage *image = [UIImage imageWithData:data];
                                                     ALAssetsLibrary *library = [[ALAssetsLibrary alloc] init];
                                                     [library writeImageToSavedPhotosAlbum:image.CGImage
                                                                               orientation:image.imageOrientation
                                                                           completionBlock:^(NSURL *assetURL, NSError *error) {
                                                                               NSLog(@"saved");
                                                                           }];
                                                 }];
</code></pre>

<p>```</p>

<h3>AVCaptureVideoDataOutput, AVCaptureAudioDataOutput</h3>

<p>デリゲートメソッドを利用して非圧縮状態のビデオフレーム情報を提供します。
そのフレーム情報は<code>CMSampleBufferRef</code>という形式で送信されてきてそれを利用して動画の保存やら編集やらを行うことができます。
このときにビデオとオーディオは別で処理しないといけません。（デリゲートメソッドが分かれているように）
主にビデオもオーディオも両方扱うときは<code>– captureOutput:didOutputSampleBuffer:fromConnection:</code>でどちらのメディアなのか区別して処理を行うような実装になります。
さらに細かい処理を行っていく場合はこちらのクラスを実装しないといけないですね。例えば、リアルタイムに動画を編集したりだとか、Vineみたいにタッチしているときは動画の保存し続けるような処理でしょうか。</p>

<ul>
<li>AVCaptureVideoDataOutputSampleBufferDelegate</li>
</ul>


<p><code>
Managing Sample Buffer Behavior
– captureOutput:didOutputSampleBuffer:fromConnection:
– captureOutput:didDropSampleBuffer:fromConnection:
</code></p>

<ul>
<li>AVCaptureAudioDataOutputSampleBufferDelegat</li>
</ul>


<p><code>
Delegate Methods
– captureOutput:didOutputSampleBuffer:fromConnection:
</code></p>

<h2>まとめ</h2>

<p>メディアキャプチャの出力先の実装について調べてみました。目的に応じてどれを使うかは選択していけばいいのですが、よりユーザにインタラクティブなインタフェースや体験をさせるにはそれ相応に細かい実装していかなければいけないこともわかりました。何度も言っているかもしれないけどメディア系の実装は大変です。。。</p>

<h2>ref</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/AVFoundationPG.pdf">AV Foundation公式ドキュメント</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/Reference/Reference.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput">AVCaptureMovieFileOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/Reference/Reference.html">AVCaptureFileOutputRecordingDelegate</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/Reference/Reference.html">AVCaptureVideoDataOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html#//apple_ref/occ/intfm/AVCaptureVideoDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:">AVCaptureVideoDataOutputSampleBufferDelegate</a></li>
<li><a href="https://developer.apple.com/library/mac/documentation/AVFoundation/Reference/AVCaptureAudioDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html">AVCaptureAudioDataOutputSampleBufferDelegate</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
