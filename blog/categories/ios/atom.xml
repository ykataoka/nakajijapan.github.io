<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[categories: iOS | Nakajijapan]]></title>
  <link href="http://nakajijapan.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://nakajijapan.github.io/"/>
  <updated>2014-05-19T23:51:05+09:00</updated>
  <id>http://nakajijapan.github.io/</id>
  <author>
    <name><![CDATA[nakajijapan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[AVAudioMixでオーディオの調節を行う]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/05/01/about-audiomix/"/>
    <updated>2014-05-01T11:56:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/05/01/about-audiomix</id>
    <content type="html"><![CDATA[<p>先日、<a href="https://itunes.apple.com/jp/app/the-ghost-movie-app-look-again/id696530211?mt=8">心霊動画アプリ「もう一度ご覧いただこう」</a>のアップデートしました。
その修正内容を書き綴ろうと思います。この動画アプリでは、録画した動画の同じタイムライン上にナレーションが入るのですが、あまりに録画した動画がにぎやかすぎるとナレーションがほとんど聞き取れない問題がありました。
対策としては、ナレーションの音声はそのままにして録画した動画の音声を小さくするように調整すること。<code>AVAudioMix</code>、<code>AVMutableAudioMixInputParameters</code>をうまく利用することで解決しました。</p>

<h2>実装</h2>

<p>基本的には対象のメディア情報に対して、オーディオを制御する用の<code>AVMutableAudioMixInputParameters</code>を利用します。</p>

<p>```obj-c
AVMutableComposition avComposition;</p>

<p>(snip)</p>

<p>// movie sound
AVMutableCompositionTrack* compositionAudioTrack = [avComposition addMutableTrackWithMediaType:AVMediaTypeAudio</p>

<pre><code>                                                                          preferredTrackID:kCMPersistentTrackID_Invalid];
</code></pre>

<p>// オーディオのメディア情報をもとにオーディオミックス用のパラメータを生成する
AVMutableAudioMixInputParameters *mixInputParameters = [AVMutableAudioMixInputParameters</p>

<pre><code>                                                    audioMixInputParametersWithTrack:audioTrack];
</code></pre>

<p>// 音量の調整
[mixInputParameters setVolumeRampFromStartVolume:0.2f toEndVolume:0.2f</p>

<pre><code>                              timeRange:CMTimeRangeMake(kCMTimeZero, videoAsset.duration)];
</code></pre>

<p>// 入力パラメータをオーディオミックスに渡す。
mutableAudioMix = [AVMutableAudioMix audioMix];
mutableAudioMix.inputParameters = @[mixInputParameters];
```</p>

<h4>&ndash; (BOOL)getVolumeRampForTime:(CMTime)time startVolume:(float <em>)startVolume endVolume:(float </em>)endVolume timeRange:(CMTimeRange *)timeRange</h4>

<p>これでどの時間からどのくらいの間、音量をどのくらいにするのかの設定を行えるので、ここでその制御を行います。自分の場合はずっと音量を低くできればよかったので上記のような設定になっています。</p>

<p>```obj-c
AVAssetExportSession *assetExportSession;</p>

<p>(snip)</p>

<p>// 音声の制御
assetExportSession.audioMix = mutableAudioMix;
```</p>

<p>設定が終われば、<code>AVAssetExportSession</code>の<code>audioMix</code>に格納すれば、設定が反映されているはずです。
これでナレーションが動画がうるさくてもしっかり聞こえるようになったので、よりテレビ番組のような感じに近づけられたかと思います！！！</p>

<h2>Resources</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableAudioMix_Class/Reference/Reference.html#//apple_ref/doc/uid/TP40009740">AVMutableAudioMix Class</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioMixInputParameters_Class/Reference/Reference.html#//apple_ref/occ/cl/AVAudioMixInputParameters">AVAudioMixInputParameters Class</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVCompositionDebugVieweriOS使ってみた]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/04/23/debug-of-a-video-editing/"/>
    <updated>2014-04-23T10:50:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/04/23/debug-of-a-video-editing</id>
    <content type="html"><![CDATA[<script async class="speakerdeck-embed" data-id="e6b31510ad7401311d323e814ed3dd4c" data-ratio="1.33333333333333" src="http://nakajijapan.github.io//speakerdeck.com/assets/embed.js"></script>


<p>先日ですが、mixiさんで開催されたスマホアプリ開発勉強会 <a href="http://atnd.org/events/49386">app道場 #2</a>に参加して、LTしてきました。</p>

<p>今回は、WWDC 2013 Session Videosの<code>Advanced Editing with AV Foundation</code>をみていたときにまだ試していないサンプルがあったのと、ビデオ編集時に使えそうなテストプログラムだったので試しに使ってみたときの話をしてきました。
「Debug of a video editing」です。サンプルプログラムのタイトルは<code>AVCompositionDebugVieweriOS</code>です。</p>

<p>これは、ビデオ編集時に大きく<code>AVComposition</code>,<code>AVVideoComposition</code>,<code>AVAudioMix</code>なるクラスを利用するのですが、その構造を可視化してくれるプログラムでした。
普段は、生成した後に単純に目的の動画が完成しているかをただ <strong>目視</strong> しているだけだったのでサンプレプログラムなのに大変助かるプログラムでした。なので少しカスタマイズして自分で作った動画アプリ<a href="https://itunes.apple.com/jp/app/xin-ling-dong-huaapuri-mou/id696530211?mt=8&amp;ign-mpt=uo%3D4">「もう一度ご覧いただこう」</a>に組み込んでおきました。</p>

<p>発表がすべて終了した後の反省会なる飲み会が、参加者全員？といろいろおしゃべりもできて大変有意義な時間を過ごさせていただきました。
mixiさん、@punchdrunkerさん、参加された皆さん楽しい時間をありがとうございました！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[メディアキャプチャの出力先の実装について]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/04/07/about-avcaptureinput/"/>
    <updated>2014-04-07T02:10:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/04/07/about-avcaptureinput</id>
    <content type="html"><![CDATA[<p><code>AV Foundation</code>を利用して写真の撮影やはたまた動画の撮影を行う場合は、カメラデバイスに接続して映像をリアルタイムに表示させたい場合があるかと思います。</p>

<p><img src="/images/posts/2014-04-06-01.png" alt="AV Foundation" /></p>

<p>そんなときは<code>AVCaptureSession</code>を利用してメディアキャプチャの実装の準備をを行い、入力先・出力先はどうするかという設定を行うのですが、今回はその出力部分について何パターンか方法があったので調べてみました。</p>

<h3>Video Output</h3>

<ul>
<li>AVCaptureMovieFileOutput

<ul>
<li>ムービーファイルを出力するときに利用する</li>
</ul>
</li>
<li>AVCaptureVideoDataOutput

<ul>
<li>キャプチャ中のビデオフレームを処理するときに利用する</li>
</ul>
</li>
<li>AVCaptureStillImageOutput

<ul>
<li>付随するメタデータを使用して静止画像をキャプチャするときに利用する</li>
</ul>
</li>
</ul>


<h3>Audio Output</h3>

<ul>
<li>AVCaptureAudioDataOutput

<ul>
<li>キャプチャ中のオーディオフレームを処理するときに利用する</li>
</ul>
</li>
</ul>


<h2>使い方</h2>

<h3>AVCaptureMovieFileOutput</h3>

<p>ムービーファイルを保存するのに必要な処理を提供してくれるクラス。</p>

<p>録画するときは</p>

<p><code>obj-c
//録画開始
[captureMovieFileOutput startRecordingToOutputFileURL:outputURL recordingDelegate:self];
</code></p>

<p>のように始めることができます。</p>

<p>必要に応じて<code>AVCaptureFileOutputRecordingDelegate</code>オプションを利用して、各タイミングに応じた処理を行います。</p>

<ul>
<li>AVCaptureFileOutputRecordingDelegate</li>
</ul>


<p>```
Delegate Methods</p>

<p>– captureOutput:didStartRecordingToOutputFileAtURL:fromConnections:
– captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:
```</p>

<p>前者は録画を開始したときに、後者は終了したときに呼び出されるメソッドです。例えば、<code>captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:</code>の時に録画したムービーファイルをデバイスに保存する等の処理が行えることができます。</p>

<h4>AVCaptureStillImageOutput</h4>

<p>画像をキャプチャするときに利用して、出力は以下のように実装します。</p>

<p>```
AVCaptureConnection *connection = [[captureStillImageOutput connections] lastObject];
[captureStillImageOutput captureStillImageAsynchronouslyFromConnection:connection</p>

<pre><code>                                                 completionHandler:^(CMSampleBufferRef imageDataSampleBuffer, NSError *error) {
                                                     NSData *data = [AVCaptureStillImageOutput jpegStillImageNSDataRepresentation:imageDataSampleBuffer];
                                                     UIImage *image = [UIImage imageWithData:data];
                                                     ALAssetsLibrary *library = [[ALAssetsLibrary alloc] init];
                                                     [library writeImageToSavedPhotosAlbum:image.CGImage
                                                                               orientation:image.imageOrientation
                                                                           completionBlock:^(NSURL *assetURL, NSError *error) {
                                                                               NSLog(@"saved");
                                                                           }];
                                                 }];
</code></pre>

<p>```</p>

<h3>AVCaptureVideoDataOutput, AVCaptureAudioDataOutput</h3>

<p>デリゲートメソッドを利用して非圧縮状態のビデオフレーム情報を提供します。
そのフレーム情報は<code>CMSampleBufferRef</code>という形式で送信されてきてそれを利用して動画の保存やら編集やらを行うことができます。
このときにビデオとオーディオは別で処理しないといけません。（デリゲートメソッドが分かれているように）
主にビデオもオーディオも両方扱うときは<code>– captureOutput:didOutputSampleBuffer:fromConnection:</code>でどちらのメディアなのか区別して処理を行うような実装になります。
さらに細かい処理を行っていく場合はこちらのクラスを実装しないといけないですね。例えば、リアルタイムに動画を編集したりだとか、Vineみたいにタッチしているときは動画の保存し続けるような処理でしょうか。</p>

<ul>
<li>AVCaptureVideoDataOutputSampleBufferDelegate</li>
</ul>


<p><code>
Managing Sample Buffer Behavior
– captureOutput:didOutputSampleBuffer:fromConnection:
– captureOutput:didDropSampleBuffer:fromConnection:
</code></p>

<ul>
<li>AVCaptureAudioDataOutputSampleBufferDelegat</li>
</ul>


<p><code>
Delegate Methods
– captureOutput:didOutputSampleBuffer:fromConnection:
</code></p>

<h2>まとめ</h2>

<p>メディアキャプチャの出力先の実装について調べてみました。目的に応じてどれを使うかは選択していけばいいのですが、よりユーザにインタラクティブなインタフェースや体験をさせるにはそれ相応に細かい実装していかなければいけないこともわかりました。何度も言っているかもしれないけどメディア系の実装は大変です。。。</p>

<h2>ref</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/AVFoundationPG.pdf">AV Foundation公式ドキュメント</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/Reference/Reference.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput">AVCaptureMovieFileOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/Reference/Reference.html">AVCaptureFileOutputRecordingDelegate</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/Reference/Reference.html">AVCaptureVideoDataOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html#//apple_ref/occ/intfm/AVCaptureVideoDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:">AVCaptureVideoDataOutputSampleBufferDelegate</a></li>
<li><a href="https://developer.apple.com/library/mac/documentation/AVFoundation/Reference/AVCaptureAudioDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html">AVCaptureAudioDataOutputSampleBufferDelegate</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[IRKit + iBeacon]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/03/29/irkit-plus-ibeacon/"/>
    <updated>2014-03-29T16:32:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/03/29/irkit-plus-ibeacon</id>
    <content type="html"><![CDATA[<p>先日IRKitとEstimote Beaconsを購入しまして、せっかくなんでこの二つ合わせてなにかできないかなーと思いまして
自宅で活用できるものを作ってみました。</p>

<h4>IRKit</h4>

<p><a href="http://www.amazon.co.jp/IRKit-001-IRKit-iPhone-iPad%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E5%A4%96%E5%87%BA%E5%85%88%E3%81%8B%E3%82%89%E3%82%A8%E3%82%A2%E3%82%B3%E3%83%B3%E7%AD%89%E3%81%AE%E5%AE%B6%E9%9B%BB%E3%82%92%E6%93%8D%E4%BD%9C%E3%81%A7%E3%81%8D%E3%82%8B%E5%AD%A6%E7%BF%92%E3%83%AA%E3%83%A2%E3%82%B3%E3%83%B3/dp/B00H91KK26%3FSubscriptionId%3D0AVSM5SVKRWTFMG7ZR82%26tag%3Ddaichibnejp-22%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3DB00H91KK26" target="_blank" title="IRKit - iPhone,iPadを使って外出先からエアコン等の家電を操作できる学習リモコン"><img src="http://ecx.images-amazon.com/images/I/31HuSy3ACXL.jpg" width="500" height="356" alt="IRKit - iPhone,iPadを使って外出先からエアコン等の家電を操作できる学習リモコン" /></a></p>

<h4>Estimote Beacons</h4>

<iframe src="http://nakajijapan.github.io//instagram.com/p/iDTqkYMR1I/embed/" width="612" height="710" frameborder="0" scrolling="no" allowtransparency="true"></iframe>


<p>仕様としては、</p>

<ul>
<li>beaconに近づいたら（家に帰ってきたら）テレビをつける</li>
<li>beaconから離れたら（外出したら）テレビを消す</li>
</ul>


<p>ってことを試してみました。</p>

<h2>下準備</h2>

<p>まずはそれぞれのライブラリを追加して<code>pod install</code>する。</p>

<p>SDKのインストール。</p>

<p><code>
target "irkit_ibeacon" do
  pod 'IRKit', :git =&gt; 'https://github.com/irkit/ios-sdk.git'
  pod 'EstimoteSDK'
end
</code></p>

<p>frameworkの追加。</p>

<p><code>
CoreLocation.framework
CoreBluetooth.framework
</code></p>

<h2>IRKit</h2>

<p>ここでは、IRKit SDKを利用してシグナルを登録してテーブルのセルにボタンを追加する処理を行います。</p>

<p>```obj-c
&ndash; (void)viewDidLoad
{</p>

<p>(snip..)</p>

<pre><code>// find IRKit
if ([IRKit sharedInstance].countOfReadyPeripherals == 0) {
    IRNewPeripheralViewController *vc = [[IRNewPeripheralViewController alloc] init];
    vc.delegate = self;
    [self presentViewController:vc
                       animated:YES
                     completion:^{
                         NSLog(@"presented");
                     }];


}

IRSignals *tmpSignals = [[IRSignals alloc] init];

// and add a signal to the collection
[tmpSignals loadFromStandardUserDefaultsKey:@"xxxxxxxxxxxxxxxxxxxxxx"];

for (int i = 0; i &lt; [[tmpSignals signals] count]; i++) {
    NSLog(@"%@", [[tmpSignals signals] objectAtIndex:i]);
}
</code></pre>

<p>}
```</p>

<p><code>IRNewPeripheralViewControllerDelegate</code>で信号の登録が終了したときに呼ばれるメソッドの実装を行います。
ここでは、ハード側で学習が完了した信号を<code>NSUserDefault</code>側でも保存させるようにしておきます。（実際にはそのシグナルを識別できるようにするためのハッシュ値のようなもの）</p>

<p>```obj-c</p>

<h1>pragma mark &ndash; IRNewPeripheralViewControllerDelegate</h1>

<ul>
<li><p>(void)newSignalViewController:(IRNewSignalViewController <em>)viewController
          didFinishWithSignal:(IRSignal </em>)signal {
  NSLog( @&ldquo;signal: %@&rdquo;, signal );</p>

<p>  if (signal) {
      [<em>datasource addSignalsObject:signal];
      [</em>datasource.signals saveToStandardUserDefaultsWithKey:@&ldquo;SIGNALS&rdquo;];
      [self.tableView reloadData];
  }
  [self dismissViewControllerAnimated:YES
                           completion:^{
                               NSLog(@&ldquo;dismissed&rdquo;);
                           }];
}</p></li>
</ul>


<h1>pragma mark &ndash; IRNewSignalViewControllerDelegate</h1>

<ul>
<li><p>(void)newPeripheralViewController:(IRNewPeripheralViewController <em>)viewController
          didFinishWithPeripheral:(IRPeripheral </em>)peripheral {
  NSLog( @&ldquo;peripheral: %@&rdquo;, peripheral );</p>

<p>  [self dismissViewControllerAnimated:YES
                           completion:^{
                               NSLog(@&ldquo;dismissed&rdquo;);
                           }];
}
```</p></li>
</ul>


<p>登録した信号を実行するときは以下のような実装で呼び出せるようになります。</p>

<p>```obj-c
&ndash; (void)tableView:(UITableView <em>)tableView didSelectRowAtIndexPath:(NSIndexPath </em>)indexPath {</p>

<pre><code>NSLog(@"indexPath: %@", indexPath);
[tableView deselectRowAtIndexPath:indexPath
                         animated:YES];

IRSignal *signal = [_datasource objectAtIndex:indexPath.row];

[signal sendWithCompletion:^(NSError *error) {
    NSLog(@"sent error: %@", error);
}];
</code></pre>

<p>}
```</p>

<h2>iBeacon</h2>

<p>ここでは、ビーコンで領域を観測し出たか入ったかを監視して、IRKitに信号を送る処理を行います。</p>

<p>まず、<code>ESTBeaconManager</code>をインスタンス化して<code>startMonitoringForRegion:</code> で監視を開始します。
<code>startRangingBeaconsInRegion:</code>で距離の観測を開始します。</p>

<p>```obj-c
&ndash; (void)viewDidLoad
{</p>

<p>(snip&hellip;)</p>

<pre><code>self.beaconManager = [[ESTBeaconManager alloc] init];
self.beaconManager.delegate = self;

ESTBeaconRegion *region = [[ESTBeaconRegion alloc] initWithProximityUUID:MY_ESTIMOTE_PROXIMITY_UUID
                                                                   major:36605
                                                                   minor:43679
                                                              identifier:@"EstimoteSampleRegion"];


[self.beaconManager startMonitoringForRegion:region];
[self.beaconManager startRangingBeaconsInRegion:region];
</code></pre>

<p>}
```</p>

<p>位置情報を利用した領域観測の方法と同様に、ビーコン領域の出入りのイベントをハンドリングします。その処理内でIRKitへ信号の送信する処理を実行します。
メソッドとしては、<code>locationManager:didEnterRegion:</code>と、<code>locationManager:didExitRegion:</code>をハンドリングして処理を行います。</p>

<p>```obj-c
&ndash;(void)beaconManager:(ESTBeaconManager <em>)manager didEnterRegion:(ESTBeaconRegion </em>)region
{</p>

<pre><code>IRSignals *signals = [[IRSignals alloc] init];
[signals loadFromStandardUserDefaultsKey:@"SIGNALS"];
IRSignal *signal = [signals objectAtIndex:0];

if (signal) {
    [signal sendWithCompletion:^(NSError *error) {
        [self say:[NSString  stringWithFormat:@"%@を実行しました。", signal.name]];
    }];
}

NSLog(@"beaconの近くに入りました。");
</code></pre>

<p>}</p>

<p>&ndash;(void)beaconManager:(ESTBeaconManager <em>)manager didExitRegion:(ESTBeaconRegion </em>)region
{</p>

<pre><code>IRSignals *signals = [[IRSignals alloc] init];
[signals loadFromStandardUserDefaultsKey:@"SIGNALS"];
IRSignal *signal = [signals objectAtIndex:1];

if (signal) {
    [signal sendWithCompletion:^(NSError *error) {
        [self say:[NSString  stringWithFormat:@"%@を実行しました。", signal.name]];
    }];
}

NSLog(@"beaconから離れました。");
</code></pre>

<p>}</p>

<p>```</p>

<h2>まとめ</h2>

<p>これで上記の仕様が実現できます。
今回の実装はまだIRKitにしてもEstimote Beaconsにしてもさわりの部分しか実装していないのですが、他の仕様も合わせたりするとまだまだ面白いことでできるかと思います。
最近、生活が変わるような技術がどんどんでてきていて楽しいですね♪（ただハードを購入しなきゃいけないのでお金がかかってしまいますがw）
まだまだこれからも、生活を変えるような技術がでてくると思うのでそれキャッチアップして試せるものは試していこうと思います！！</p>

<p>Enjoy Technology!!!</p>

<h2>Ref</h2>

<ul>
<li><a href="http://getirkit.com/">IRKit</a></li>
<li><a href="http://estimote.com/">Estimote Beacons</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[第５回 #potatotips に参加してきた。]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/03/17/potetotips5-nkjmoviecomposer/"/>
    <updated>2014-03-17T03:37:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/03/17/potetotips5-nkjmoviecomposer</id>
    <content type="html"><![CDATA[<p>先日ですが、#potatetips　（第五回）に参加させていただきました。
持ち時間一人５分のtips共有会なのですが、自分はAVFoundation周りの話をしてきました。
本当は実装しているときにこんなメソッド使うと便利だよとかこうすると迷走するからこうしたしたほうがいいよとかあればよかったのですが
いまいち思いつかなくて、自分でプラグイン作成して、これ使うと便利だよっ、知らなかったでしょう！！という感じに強制的にもっていった発表内容でした。
内容としては前回の発表のつづきで、以前作成したアプリの機能を切り出して、動画関連の処理を簡単に実装できるようにした（CocoaPods化）した話をしました。</p>

<script async class="speakerdeck-embed" data-id="f675b2008c7e013162b85e7a2e8ee0d7" data-ratio="1.33333333333333" src="http://nakajijapan.github.io//speakerdeck.com/assets/embed.js"></script>


<p><a href="https://github.com/nakajijapan/NKJMovieComposer">NKJMovieComposer</a></p>

<p>インストール方法は至って簡単で</p>

<p><code>
pod "NKJMovieComposer"
</code></p>

<p>でオッケーです。後は実装するファイルに<code>#import 'NKJMovieComposer.h'を追加すればOKです。</code></p>

<p>機能としては以下の機能が実装できます。</p>

<ul>
<li>動画結合</li>
<li>ワイプ</li>
<li>フェードイン・アウト</li>
</ul>


<p>詳しいことは<a href="https://github.com/nakajijapan/NKJMovieComposer">GitHub</a>をみていただくことにして、
まだエラー処理とかデバイス・OSに応じたカスタマイズがまだまだできていないので地道に実装していこうと思います。</p>

<p>最後に、発表者のみなさん、クックパッドさんおつかれさまでした！</p>
]]></content>
  </entry>
  
</feed>
