<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[categories: iOS | Nakajijapan]]></title>
  <link href="http://nakajijapan.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://nakajijapan.github.io/"/>
  <updated>2014-06-01T23:51:16-07:00</updated>
  <id>http://nakajijapan.github.io/</id>
  <author>
    <name><![CDATA[nakajijapan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[app道場#3 「よくある動画アプリのあれを実装したい」を発表した]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/06/01/app-doujou3/"/>
    <updated>2014-06-01T20:05:00-07:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/06/01/app-doujou3</id>
    <content type="html"><![CDATA[<p>２回目の参加となるiPhone and Android勉強会の<a href="http://atnd.org/events/50499">「app道場#3」</a>に参加してきました。</p>

<p>今回は、自分が酔った勢いで以下のような発言をしてしまい、大変誤解を招くような発言をしてしまいました。この場を借りて陳謝したいと思います。
そんなこんながありまして、弊社で勉強会を開催する運びとなりました。</p>

<blockquote class="twitter-tweet" lang="ja"><p>mixiさんに喧嘩売ってきた。</p>&mdash; nakajijapan (@nakajijapan) <a href="https://twitter.com/nakajijapan/statuses/458961908943446016">2014, 4月 23</a></blockquote>


<script async src="http://nakajijapan.github.io//platform.twitter.com/widgets.js" charset="utf-8"></script>


<p>詳細は<a href="http://www.vagrantup.jp/entry/2014/05/31/234116">「喧嘩を売る」</a>を参照すると良いでしょう。</p>

<blockquote><p>お分かりの通り、ここでいう「喧嘩を売る」は「次の勉強会を企画・開催する」と解釈されます。
「app道場」という字面的に少し物騒ではありますが「喧嘩」というフレーズは意外にも正鵠を射ているのではないかと思う次第です。（さすが、@nakajijapan）</p></blockquote>

<p>いいこといっています。</p>

<h2>本題</h2>

<p>さて、今回は「よくある動画アプリのあれを実装したい」というタイトルで、またもやAV Foundationの話になります。自分も動画周りの勉強してる身として、VineとかInstagramとかの動画のインターフェースの実装ってどうやるんだろうなと疑問に思っていました。
気になって気になってしょうがなかったのでちょっと実装してみようかなという想い、自分なりに考えて実装してみました。</p>

<script async class="speakerdeck-embed" data-id="52a36f80c901013130852e590c444dc4" data-ratio="1.33333333333333" src="http://nakajijapan.github.io//speakerdeck.com/assets/embed.js"></script>


<p>仕様自体は以下のようになっています。</p>

<ul>
<li>タッチが開始したら動画の保存を開始する</li>
<li>タッチが終了したら動画の保存を終了する</li>
<li>動画はファイルに保存する</li>
<li>それぞれ保存した動画を結合して一つの動画ファイルに結合する。</li>
</ul>


<p>詳細はスライドを見ていただくことにして、実際にいい感じにできたのでCocoaPodsに登録しました。</p>

<p><a href="https//github.com/nakajijapan/NKJMultiMovieCaptureView.git">NKJMultiMovieCaptureView</a></p>

<p>これが今回公開したpodで、SessionCaptureView部分にタッチして動画を保存するところまでの処理を提供しています。
実際の保存処理は前回作成したpod, <a href="https//github.com/nakajijapan/NKJMovieComposer.git">NKJMovieComposer</a>を利用することで簡単に実装することができます。</p>

<p>しかし、まだまだまだまだ汎用性に欠ける部分が多々あるので粛々と時間を見つけて改善していこうと思います。</p>

<h4>課題</h4>

<ul>
<li>現在は正方形のビデオサイズを主にテスト対象にしているのでいろんなサイズに対応できるようにする</li>
<li>動画を結合したときに最初の0.1?0.2?だけ真っ黒になる部分がでてしまう。</li>
</ul>


<h2>懇親会</h2>

<p>今回の勉強会にSlideStoryさんの中の人が来てくださって、動画周りのことでやんややんやお話を聞くとができたので大変勉強なりやした。
実装してみてやはり同じ課題にぶつかっていたそうです。あと何個か動画関連のpodを紹介してもらえたので時間を見つけて実装して見ようと思います。</p>

<ul>
<li><a href="https://github.com/rFlex/SCRecorder">SCRecorder</a></li>
<li><a href="https://github.com/BradLarson/GPUImage">GPUImage</a></li>
</ul>


<p>また、今回の勉強会の裏側で違う勉強会が開催されていた模様で自分と同じような内容の発表があり、大変シンクロを感じましたので紹介させていただきます。かなり驚きました！！！！w</p>

<iframe src="http://www.slideshare.net/slideshow/embed_code/35199041" width="427" height="356" frameborder="0" marginwidth="0" marginheight="0" scrolling="no" style="border:1px solid #CCC; border-width:1px 1px 0; margin-bottom:5px; max-width: 100%;" allowfullscreen> </iframe>


<p> <div style="margin-bottom:5px"> <strong> <a href="https://www.slideshare.net/himaratsu/6-vine" title="6秒動画アプリ Vineの作り方" target="_blank">6秒動画アプリ Vineの作り方</a> </strong> from <strong><a href="http://www.slideshare.net/himaratsu" target="_blank">Hiramatsu Ryosuke</a></strong> </div></p>

<p>最後に、勉強会準備で手伝ってくださったみなさん、app道場関係者のみなさん、本当にお疲れ様でした。また次も参加します！！</p>

<p>だれかに喧嘩売らねば！！！！！（売りません買わないでださい）</p>

<h2>Ref</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/Reference/Reference.html">AVCaptureVideoDataOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html#//apple_ref/occ/intfm/AVCaptureVideoDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:">AVCaptureVideoDataOutputSampleBufferDelegate</a></li>
<li><a href="https://developer.apple.com/library/mac/documentation/AVFoundation/Reference/AVCaptureAudioDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html">AVCaptureAudioDataOutputSampleBufferDelegate</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAssetWriter_Class/Reference/Reference.html">AVAssetWriter</a></li>
<li><a href="https://developer.apple.com/library/mac/documentation/AVFoundation/Reference/AVAssetWriterInput_Class/Reference/Reference.html">AVAssetWriterInput</a></li>
<li><a href="https://developer.apple.com/library/mac/documentation/CoreMedia/Reference/CMSampleBuffer/Reference/reference.html">CMSampleBuffer</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[動画からサムネイル画像を生成する]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/05/25/creating-thumbnailimage-from-the-movie/"/>
    <updated>2014-05-25T22:02:00-07:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/05/25/creating-thumbnailimage-from-the-movie</id>
    <content type="html"><![CDATA[<p>動画のある一部分を画像として取り出したいときがあったので調べてみたら、ドキュメントにそれっぽい記述があったので実装してみました。</p>

<p>方法としては、以下のメソッドを利用してCMTimeを利用して指定された時間に対する画像を生成します。</p>

<blockquote><p>copyCGImageAtTime:actualTime:error:</p></blockquote>

<p>実際に、メソッドにして動くようにしました。</p>

<p>```
&ndash; (UIImage<em>)createThumbnailImageWithURL:(NSURL </em>)movieURL
{</p>

<pre><code>AVAsset *asset = [[AVURLAsset alloc] initWithURL:[movieURL objectAtIndex:index] options:nil];

if ([asset tracksWithMediaCharacteristic:AVMediaTypeVideo]) {
    AVAssetImageGenerator *imageGenerator = [[AVAssetImageGenerator alloc] initWithAsset:asset];

    Float64 durationSeconds = CMTimeGetSeconds([asset duration]);
    CMTime midpoint         = CMTimeMakeWithSeconds(durationSeconds/2.0, 600);
    NSError* error          = nil;
    CMTime actualTime;

    CGImageRef halfWayImageRef = [imageGenerator copyCGImageAtTime:midpoint actualTime:&amp;actualTime error:&amp;error];

    if (halfWayImageRef != NULL) {
        UIImage* image = [[UIImage alloc]initWithCGImage:halfWayImageRef];

        // 明示的にリリースしておきます
        CGImageRelease(halfWayImageRef);
        return image;
    }
}
return nil;
</code></pre>

<p>}
```</p>

<p>動画を取り終わった後に、一時的に一覧として残しておきたいときなどに利用できますね。</p>

<h2>p.s.</h2>

<p>これは試していないのですが、以下のメソッドがあってブロックの中で複数の画像が生成できるようです。</p>

<blockquote><p>generateCGImagesAsynchronouslyForTimes:completionHandler::^(CMTime requestedTime, CGImageRef image, CMTime actualTime, AVAssetImageGeneratorResult result, NSError
*error)</p></blockquote>

<h2>ref</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/AVFoundationPG.pdf">AV Foundation Programming Guide</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVAudioMixでオーディオの調節を行う]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/05/01/about-audiomix/"/>
    <updated>2014-05-01T11:56:00-07:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/05/01/about-audiomix</id>
    <content type="html"><![CDATA[<p>先日、<a href="https://itunes.apple.com/jp/app/the-ghost-movie-app-look-again/id696530211?mt=8">心霊動画アプリ「もう一度ご覧いただこう」</a>のアップデートしました。
その修正内容を書き綴ろうと思います。この動画アプリでは、録画した動画の同じタイムライン上にナレーションが入るのですが、あまりに録画した動画がにぎやかすぎるとナレーションがほとんど聞き取れない問題がありました。
対策としては、ナレーションの音声はそのままにして録画した動画の音声を小さくするように調整すること。<code>AVAudioMix</code>、<code>AVMutableAudioMixInputParameters</code>をうまく利用することで解決しました。</p>

<h2>実装</h2>

<p>基本的には対象のメディア情報に対して、オーディオを制御する用の<code>AVMutableAudioMixInputParameters</code>を利用します。</p>

<p>```obj-c
AVMutableComposition avComposition;</p>

<p>(snip)</p>

<p>// movie sound
AVMutableCompositionTrack* compositionAudioTrack = [avComposition addMutableTrackWithMediaType:AVMediaTypeAudio</p>

<pre><code>                                                                          preferredTrackID:kCMPersistentTrackID_Invalid];
</code></pre>

<p>// オーディオのメディア情報をもとにオーディオミックス用のパラメータを生成する
AVMutableAudioMixInputParameters *mixInputParameters = [AVMutableAudioMixInputParameters</p>

<pre><code>                                                    audioMixInputParametersWithTrack:audioTrack];
</code></pre>

<p>// 音量の調整
[mixInputParameters setVolumeRampFromStartVolume:0.2f toEndVolume:0.2f</p>

<pre><code>                              timeRange:CMTimeRangeMake(kCMTimeZero, videoAsset.duration)];
</code></pre>

<p>// 入力パラメータをオーディオミックスに渡す。
mutableAudioMix = [AVMutableAudioMix audioMix];
mutableAudioMix.inputParameters = @[mixInputParameters];
```</p>

<h4>&ndash; (BOOL)getVolumeRampForTime:(CMTime)time startVolume:(float <em>)startVolume endVolume:(float </em>)endVolume timeRange:(CMTimeRange *)timeRange</h4>

<p>これでどの時間からどのくらいの間、音量をどのくらいにするのかの設定を行えるので、ここでその制御を行います。自分の場合はずっと音量を低くできればよかったので上記のような設定になっています。</p>

<p>```obj-c
AVAssetExportSession *assetExportSession;</p>

<p>(snip)</p>

<p>// 音声の制御
assetExportSession.audioMix = mutableAudioMix;
```</p>

<p>設定が終われば、<code>AVAssetExportSession</code>の<code>audioMix</code>に格納すれば、設定が反映されているはずです。
これでナレーションが動画がうるさくてもしっかり聞こえるようになったので、よりテレビ番組のような感じに近づけられたかと思います！！！</p>

<h2>Resources</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableAudioMix_Class/Reference/Reference.html#//apple_ref/doc/uid/TP40009740">AVMutableAudioMix Class</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVAudioMixInputParameters_Class/Reference/Reference.html#//apple_ref/occ/cl/AVAudioMixInputParameters">AVAudioMixInputParameters Class</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVCompositionDebugVieweriOS使ってみた]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/04/23/debug-of-a-video-editing/"/>
    <updated>2014-04-23T10:50:00-07:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/04/23/debug-of-a-video-editing</id>
    <content type="html"><![CDATA[<script async class="speakerdeck-embed" data-id="e6b31510ad7401311d323e814ed3dd4c" data-ratio="1.33333333333333" src="http://nakajijapan.github.io//speakerdeck.com/assets/embed.js"></script>


<p>先日ですが、mixiさんで開催されたスマホアプリ開発勉強会 <a href="http://atnd.org/events/49386">app道場 #2</a>に参加して、LTしてきました。</p>

<p>今回は、WWDC 2013 Session Videosの<code>Advanced Editing with AV Foundation</code>をみていたときにまだ試していないサンプルがあったのと、ビデオ編集時に使えそうなテストプログラムだったので試しに使ってみたときの話をしてきました。
「Debug of a video editing」です。サンプルプログラムのタイトルは<code>AVCompositionDebugVieweriOS</code>です。</p>

<p>これは、ビデオ編集時に大きく<code>AVComposition</code>,<code>AVVideoComposition</code>,<code>AVAudioMix</code>なるクラスを利用するのですが、その構造を可視化してくれるプログラムでした。
普段は、生成した後に単純に目的の動画が完成しているかをただ <strong>目視</strong> しているだけだったのでサンプレプログラムなのに大変助かるプログラムでした。なので少しカスタマイズして自分で作った動画アプリ<a href="https://itunes.apple.com/jp/app/xin-ling-dong-huaapuri-mou/id696530211?mt=8&amp;ign-mpt=uo%3D4">「もう一度ご覧いただこう」</a>に組み込んでおきました。</p>

<p>発表がすべて終了した後の反省会なる飲み会が、参加者全員？といろいろおしゃべりもできて大変有意義な時間を過ごさせていただきました。
mixiさん、@punchdrunkerさん、参加された皆さん楽しい時間をありがとうございました！</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[メディアキャプチャの出力先の実装について]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/04/07/about-avcaptureinput/"/>
    <updated>2014-04-07T02:10:00-07:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/04/07/about-avcaptureinput</id>
    <content type="html"><![CDATA[<p><code>AV Foundation</code>を利用して写真の撮影やはたまた動画の撮影を行う場合は、カメラデバイスに接続して映像をリアルタイムに表示させたい場合があるかと思います。</p>

<p><img src="/images/posts/2014-04-06-01.png" alt="AV Foundation" /></p>

<p>そんなときは<code>AVCaptureSession</code>を利用してメディアキャプチャの実装の準備をを行い、入力先・出力先はどうするかという設定を行うのですが、今回はその出力部分について何パターンか方法があったので調べてみました。</p>

<h3>Video Output</h3>

<ul>
<li>AVCaptureMovieFileOutput

<ul>
<li>ムービーファイルを出力するときに利用する</li>
</ul>
</li>
<li>AVCaptureVideoDataOutput

<ul>
<li>キャプチャ中のビデオフレームを処理するときに利用する</li>
</ul>
</li>
<li>AVCaptureStillImageOutput

<ul>
<li>付随するメタデータを使用して静止画像をキャプチャするときに利用する</li>
</ul>
</li>
</ul>


<h3>Audio Output</h3>

<ul>
<li>AVCaptureAudioDataOutput

<ul>
<li>キャプチャ中のオーディオフレームを処理するときに利用する</li>
</ul>
</li>
</ul>


<h2>使い方</h2>

<h3>AVCaptureMovieFileOutput</h3>

<p>ムービーファイルを保存するのに必要な処理を提供してくれるクラス。</p>

<p>録画するときは</p>

<p><code>obj-c
//録画開始
[captureMovieFileOutput startRecordingToOutputFileURL:outputURL recordingDelegate:self];
</code></p>

<p>のように始めることができます。</p>

<p>必要に応じて<code>AVCaptureFileOutputRecordingDelegate</code>オプションを利用して、各タイミングに応じた処理を行います。</p>

<ul>
<li>AVCaptureFileOutputRecordingDelegate</li>
</ul>


<p>```
Delegate Methods</p>

<p>– captureOutput:didStartRecordingToOutputFileAtURL:fromConnections:
– captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:
```</p>

<p>前者は録画を開始したときに、後者は終了したときに呼び出されるメソッドです。例えば、<code>captureOutput:didFinishRecordingToOutputFileAtURL:fromConnections:error:</code>の時に録画したムービーファイルをデバイスに保存する等の処理が行えることができます。</p>

<h4>AVCaptureStillImageOutput</h4>

<p>画像をキャプチャするときに利用して、出力は以下のように実装します。</p>

<p>```
AVCaptureConnection *connection = [[captureStillImageOutput connections] lastObject];
[captureStillImageOutput captureStillImageAsynchronouslyFromConnection:connection</p>

<pre><code>                                                 completionHandler:^(CMSampleBufferRef imageDataSampleBuffer, NSError *error) {
                                                     NSData *data = [AVCaptureStillImageOutput jpegStillImageNSDataRepresentation:imageDataSampleBuffer];
                                                     UIImage *image = [UIImage imageWithData:data];
                                                     ALAssetsLibrary *library = [[ALAssetsLibrary alloc] init];
                                                     [library writeImageToSavedPhotosAlbum:image.CGImage
                                                                               orientation:image.imageOrientation
                                                                           completionBlock:^(NSURL *assetURL, NSError *error) {
                                                                               NSLog(@"saved");
                                                                           }];
                                                 }];
</code></pre>

<p>```</p>

<h3>AVCaptureVideoDataOutput, AVCaptureAudioDataOutput</h3>

<p>デリゲートメソッドを利用して非圧縮状態のビデオフレーム情報を提供します。
そのフレーム情報は<code>CMSampleBufferRef</code>という形式で送信されてきてそれを利用して動画の保存やら編集やらを行うことができます。
このときにビデオとオーディオは別で処理しないといけません。（デリゲートメソッドが分かれているように）
主にビデオもオーディオも両方扱うときは<code>– captureOutput:didOutputSampleBuffer:fromConnection:</code>でどちらのメディアなのか区別して処理を行うような実装になります。
さらに細かい処理を行っていく場合はこちらのクラスを実装しないといけないですね。例えば、リアルタイムに動画を編集したりだとか、Vineみたいにタッチしているときは動画の保存し続けるような処理でしょうか。</p>

<ul>
<li>AVCaptureVideoDataOutputSampleBufferDelegate</li>
</ul>


<p><code>
Managing Sample Buffer Behavior
– captureOutput:didOutputSampleBuffer:fromConnection:
– captureOutput:didDropSampleBuffer:fromConnection:
</code></p>

<ul>
<li>AVCaptureAudioDataOutputSampleBufferDelegat</li>
</ul>


<p><code>
Delegate Methods
– captureOutput:didOutputSampleBuffer:fromConnection:
</code></p>

<h2>まとめ</h2>

<p>メディアキャプチャの出力先の実装について調べてみました。目的に応じてどれを使うかは選択していけばいいのですが、よりユーザにインタラクティブなインタフェースや体験をさせるにはそれ相応に細かい実装していかなければいけないこともわかりました。何度も言っているかもしれないけどメディア系の実装は大変です。。。</p>

<h2>ref</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/AVFoundationPG.pdf">AV Foundation公式ドキュメント</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureMovieFileOutput_Class/Reference/Reference.html#//apple_ref/occ/cl/AVCaptureMovieFileOutput">AVCaptureMovieFileOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureFileOutputRecordingDelegate_Protocol/Reference/Reference.html">AVCaptureFileOutputRecordingDelegate</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutput_Class/Reference/Reference.html">AVCaptureVideoDataOutput</a></li>
<li><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVCaptureVideoDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html#//apple_ref/occ/intfm/AVCaptureVideoDataOutputSampleBufferDelegate/captureOutput:didOutputSampleBuffer:fromConnection:">AVCaptureVideoDataOutputSampleBufferDelegate</a></li>
<li><a href="https://developer.apple.com/library/mac/documentation/AVFoundation/Reference/AVCaptureAudioDataOutputSampleBufferDelegate_Protocol/Reference/Reference.html">AVCaptureAudioDataOutputSampleBufferDelegate</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
