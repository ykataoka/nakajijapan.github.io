<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[categories: ios | Nakajijapan]]></title>
  <link href="http://nakajijapan.github.io/blog/categories/ios/atom.xml" rel="self"/>
  <link href="http://nakajijapan.github.io/"/>
  <updated>2014-02-17T03:24:21+09:00</updated>
  <id>http://nakajijapan.github.io/</id>
  <author>
    <name><![CDATA[nakajijapan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Confrence With Developer2に参加してLTしてきた]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/02/02/conference-with-developers-2014/"/>
    <updated>2014-02-02T23:32:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/02/02/conference-with-developers-2014</id>
    <content type="html"><![CDATA[<p>今回、Confrence With Developer2に参加してきました。
iOSに携わる、すべての開発者に向けたカンファレンスイベントだったのですが
いろいろ収穫があり、たくさん勉強させていただきました。</p>

<p>そんな僕もここでLTをしてきたのでのせときます。以前は、MA9でも発表している
ですがあくまでプロダクトとして発表だったのと今回は技術的なカンファレンス
なので技術的側面に焦点をあてて発表させていただきました。</p>

<p>話した内容はアプリでがんがん利用していた<code>AV Foundation</code>の話をしてきました。</p>

<script async class="speakerdeck-embed" data-id="ebaab3406d820131c6665e5071095c76" data-ratio="1.33333333333333" src="http://nakajijapan.github.io//speakerdeck.com/assets/embed.js"></script>


<p>(発表時にディスプレイコネクタが認識しなくて発狂していたところ<code>@tamotamago</code>さんにパソコンを貸していただき大変感謝です！)</p>

<p>自分でもいろいろとメモっていたのですが以下のサイトがよく纏められていたのでご参照ください。</p>

<h3>Developers.IO</h3>

<ul>
<li><a href="http://dev.classmethod.jp/smartphone/ios-confwd2/">http://dev.classmethod.jp/smartphone/ios-confwd2/</a></li>
</ul>


<h3>Togetter</h3>

<ul>
<li><a href="http://togetter.com/li/623779">http://togetter.com/li/623779</a></li>
</ul>


<h3>Ustream録画</h3>

<ul>
<li><a href="http://www.ustream.tv/recorded/43319245">http://www.ustream.tv/recorded/43319245</a></li>
<li><a href="http://www.ustream.tv/recorded/43323576">http://www.ustream.tv/recorded/43323576</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AVMutableVideoCompositionLayerInstructionの使い方]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/01/19/how-to-avmutablevideocompositionlayerinstruction/"/>
    <updated>2014-01-19T19:32:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/01/19/how-to-avmutablevideocompositionlayerinstruction</id>
    <content type="html"><![CDATA[<p>ビデオ用の<code>AVMutableCompositionTrack</code>にレイヤーを持たせることができます。
それが<code>AVMutableVideoCompositionLayerInstruction</code>なんですがこれが意外と
簡単に動画に対して<code>CGAffineTransform</code>を指定することで動画の移動・回転・拡大縮小
ができたりフェードイン・アウトの設定ができたのでメモ。</p>

<p>まずは対象のトラックをもとにして、レイヤーをインスタンス化します。</p>

<p><code>+ (AVMutableVideoCompositionLayerInstruction *)videoCompositionLayerInstructionWithAssetTrack:(AVAssetTrack *)track</code></p>

<p>```obj-c</p>

<pre><code>AVMutableVideoCompositionLayerInstruction *layerInstruction;
layerInstruction = [AVMutableVideoCompositionLayerInstruction
                    videoCompositionLayerInstructionWithAssetTrack:compositionVideoTrack];
</code></pre>

<p>```</p>

<h3>TransForm</h3>

<p><code>- (void)setTransformRampFromStartTransform:(CGAffineTransform)startTransform toEndTransform:(CGAffineTransform)endTransform timeRange:(CMTimeRange)timeRange</code></p>

<p>この例だと、３秒間左から動画が移動してきます。</p>

<p>```obj-c</p>

<pre><code>// transition
CGAffineTransform rotateStart, rotateEnd;
startTime    = kCMTimeZero;
timeDuration = CMTimeMake(3, 1);
rotateStart  = CGAffineTransformMakeScale(1, 1);
rotateStart  = CGAffineTransformMakeTranslation(-720, 0);
rotateEnd    = CGAffineTransformTranslate(rotateStart, 720, 0);
[layerInstruction setTransformRampFromStartTransform:rotateStart
                                      toEndTransform:rotateEnd
                                           timeRange:CMTimeRangeMake(startTime, timeDuration)];
</code></pre>

<p>```</p>

<h3>Opacity</h3>

<p><code>- (void)setOpacityRampFromStartOpacity:(float)startOpacity toEndOpacity:(float)endOpacity timeRange:(CMTimeRange)timeRange</code></p>

<p>この例だと最初から３秒間動画がフェードインしてきて最後の３秒間動画をフェードアウトします。</p>

<p>```obj-c</p>

<pre><code>// fade in
CMTime startTime, timeDuration;
startTime    = kCMTimeZero;
timeDuration = CMTimeMake(3, 1);
[layerInstruction setOpacityRampFromStartOpacity:0.0
                                    toEndOpacity:1.0
                                       timeRange:CMTimeRangeMake(startTime, timeDuration)];


// fade out
startTime    = CMTimeSubtract(videoComposition.currentTimeDuration, CMTimeMake(3, 1));
timeDuration = CMTimeMake(3, 1);
[layerInstruction setOpacityRampFromStartOpacity:1.0
                                    toEndOpacity:0.0
                                       timeRange:CMTimeRangeMake(startTime, timeDuration)];
</code></pre>

<p>```</p>

<h2>まとめ</h2>

<p>Appleのドキュメントでこのクラスの説明をみると動画処理にしては結構シンプルだったので何個か試してみたのでした。
ただ、<code>CMTime</code>関連・<code>CGAffineTransform</code>関連の知識はないと結構何がなんだかわからなくなるのであらかじめ知識として
蓄えておくことはは必須ですね。</p>

<h2>Ref</h2>

<p><a href="https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionLayerInstruction_Class/Reference/Reference.html#//apple_ref/occ/instm/AVMutableVideoCompositionLayerInstruction/setTransformRampFromStartTransform:toEndTransform:timeRange:">https://developer.apple.com/library/ios/documentation/AVFoundation/Reference/AVMutableVideoCompositionLayerInstruction_Class/Reference/Reference.html#//apple_ref/occ/instm/AVMutableVideoCompositionLayerInstruction/setTransformRampFromStartTransform:toEndTransform:timeRange:</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[動画関連の処理をする上で注意すること]]></title>
    <link href="http://nakajijapan.github.io/blog/2014/01/19/avfoundation-checkpoint/"/>
    <updated>2014-01-19T19:32:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2014/01/19/avfoundation-checkpoint</id>
    <content type="html"><![CDATA[<p>以前の記事でも書いたのですが、<a href="https://itunes.apple.com/jp/app/the-ghost-movie-app-look-again/id696530211?mt=8">心霊動画アプリ「もう一度ご覧いただこう」</a>というアプリを作成したときにいろいろと苦労したことがあったので、それを書き留めておこうと思います。</p>

<h3>回転問題</h3>

<p>そのまま動画を保存すると270度になってしまっているので回転する必要があった</p>

<p>```</p>

<pre><code>CGAffineTransform transformVideo       = CGAffineTransformMakeTranslation(videoTrack.naturalSize.height, 0.0);
CGAffineTransform transformVideoRotate = CGAffineTransformRotate(transformVideo, M_PI * 0.5);
CGAffineTransform transformVideoMove   = CGAffineTransformTranslate(transformVideoRotate, -300, 0);
[layerInstruction setTransform: transformVideoMove atTime:kCMTimeZero];
</code></pre>

<p>```</p>

<p>結構、簡単で地味なことだったのですが、こういうことが原因で数日迷走していた日があったのでもう迷走しない為に。。。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[iOS勉強会 iBeaconでできること　に参加してきた]]></title>
    <link href="http://nakajijapan.github.io/blog/2013/12/12/study-ibeacon/"/>
    <updated>2013-12-12T19:06:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2013/12/12/study-ibeacon</id>
    <content type="html"><![CDATA[<p>Developer&rsquo;s IOさんが主催する勉強会に参加してきました。
<a href="http://connpass.com/event/4225/">http://connpass.com/event/4225/</a></p>

<p>iOS7の発表がでてから<code>iBeacon</code>に興味を持ち出して、このにゅ〜てくのろじ〜を面白いことにつかえないかと調査していたのですが、今回もっと知識を深めるべく勉強会にさんかしてきました。そのメモを残しておこうと思います。</p>

<h1>【初級】iBeaconの基礎のキ</h1>

<p>開発者以外にひとでも分かるようにiBeaconというものをざっくり説明してました。</p>

<h2>iBeaconとは</h2>

<ul>
<li>BLEを利用している</li>
<li>Bluetooth onにしないといけない</li>
<li>CoreLocation.framework</li>
<li>iOS7がインストールされるからってBLEが利用されるわけではない

<ul>
<li>iOS7 + BLEが利用できるっていうのが重要</li>
</ul>
</li>
</ul>


<p>ざっくり定義すると</p>

<p><code>BLEを利用したAppleの距離と領域観測のサービス</code></p>

<p>です。</p>

<h2>利用したサービス</h2>

<ul>
<li>AppleStore

<ul>
<li>テーブルに近づくとiPhoneの紹介や下取りの情報が表示される</li>
</ul>
</li>
<li>Macy&rsquo;s

<ul>
<li>入店検知してクーポンの取得、おすすめ商品の紹介</li>
<li>O2O施策としてだいぶ確率されているようです</li>
</ul>
</li>
<li>スマポ

<ul>
<li>来店ポイントがたまる</li>
<li>日本でしっかりベジネスにしているサービスですね</li>
</ul>
</li>
<li>MLB

<ul>
<li>入場用のバーコード</li>
<li>point</li>
<li>クーポン</li>
<li>本日の対戦情報を表示してくれる</li>
<li>自分の座席情報を教えてくれる</li>
<li>とはいえ、現在まだベータ運用しているらしく、本番運用はまだのようですね</li>
</ul>
</li>
</ul>


<h2>よく利用される用語</h2>

<ul>
<li>Beacon

<ul>
<li>電子情報を受信して現在情報を知る電子機器のことをいうらしい

<ul>
<li><a href="http://e-words.jp/w/E38393E383BCE382B3E383B3.html">http://e-words.jp/w/E38393E383BCE382B3E383B3.html</a></li>
</ul>
</li>
</ul>
</li>
<li>BLE

<ul>
<li>2.4GHz の無線を使った近距離無線通信規格</li>
<li>少ない電池でもいままでよりも長くバッテリーが持つようにつくられている</li>
</ul>
</li>
<li>Advertise

<ul>
<li>アドバタイズ：ある機器が別の機器に管理情報を伝達すること</li>
<li>proximity UUID（企業）</li>
<li>major（建物）</li>
<li>minor（フロア）</li>
</ul>
</li>
<li>リージョン監視

<ul>
<li>領域の入出チェック（虜域観測）</li>
<li>リージョン=UUID</li>
<li>出るイベントはドキュメント情報は２０秒っぽいが実際は３０秒かかるっぽい</li>
</ul>
</li>
<li>Ranging

<ul>
<li>エリア内のBeaconの情報を取得（距離推定）</li>
<li>Beacon距離

<ul>
<li>proximity

<ul>
<li>距離判定は４つとれる</li>
<li>相対的に変わる</li>
</ul>
</li>
<li>accuracy

<ul>
<li>位置情報取得の精度を指定</li>
<li>もちろん精度が良いほど正確だが電池の量も多い</li>
<li>領域はだいたいのもの</li>
</ul>
</li>
<li>rssi

<ul>
<li>Received Signal Strength Indication</li>
<li>無線通信機器が受信する信号の強度を測定するための回路または信号</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>電波法！！！！

<ul>
<li>技適マークがついていない無線機は電波法違反らしい</li>
<li>estimoteのビーコン端末は日本実運用すると電波法違反になるぽいですね</li>
</ul>
</li>
</ul>


<h2>iBeaconを使ってみよう</h2>

<p>iBeaconアプリの作り方を二つのキーワードで説明しています。
以下の二つをもとに実装を行っていく</p>

<ul>
<li>距離観測（Ranging）

<ul>
<li>各Beaconとの距離を約１秒おきに監視する</li>
<li>それぞれの領域をみる</li>
</ul>
</li>
<li><p>領域観測のた仕様</p>

<ul>
<li>Beaconグループで作成される領域</li>
</ul>
</li>
<li><p>uuidgen
ビーコン端末に固有のID振らないといけないのでコマンドを利用して生成したりできます
<code>
$ uuidgen
8649794A-0C6C-4D82-99F2-7084D6C231B1
</code></p></li>
</ul>


<h1>Beaconアプリの作り方</h1>

<p>具体的にコードレベルでどんな風に作成されるのか、ハマりどころを説明していただきました。</p>

<ul>
<li>具体的に実装がでてきたコード

<ul>
<li><a href="https://github.com/suwa-yuki/BeaconSample">https://github.com/suwa-yuki/BeaconSample</a></li>
</ul>
</li>
<li>ハマりどころ

<ul>
<li>startMonitoringForRegionはバックグラウンドおk</li>
</ul>
</li>
</ul>


<h1>LTで話されたことメモ</h1>

<ul>
<li>Androidでの確認はほぼできない

<ul>
<li>Nexus5だと大丈夫</li>
<li>あとは海外の端末を輸入する</li>
<li>iPhoneは無論OK</li>
</ul>
</li>
</ul>


<h1>まとめ</h1>

<p>今回はiBeaconを使って開発する為の基本知識をベースとした勉強会でたくさん学ばしていいただきました。
まだまだ勉強不足なところはあるけれど、今回の勉強会でだいぶもやもやしていたところが解消したので次の開発に活かそうと思います。
しかし、つい先日estimoteのbeacon端末を購入したのですが技適マークがついていないのは残念です。。。ビジネスで
利用するとしたらまた別の端末（日本の電波法に準拠したハード製品）を購入してやらないとだめですね。。。</p>

<h1>参考スライド</h1>

<p><a href="http://www.slideshare.net/RikitakeOohashi/i-beacon-29135007">http://www.slideshare.net/RikitakeOohashi/i-beacon-29135007</a>
<a href="http://www.slideshare.net/yuki0211s/i-beacon-29135560">http://www.slideshare.net/yuki0211s/i-beacon-29135560</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[AV Foundation Frameworkを利用して動画の結合を行う]]></title>
    <link href="http://nakajijapan.github.io/blog/2013/10/22/how-to-combine-some-movies-with-avfoundation/"/>
    <updated>2013-10-22T19:06:00+09:00</updated>
    <id>http://nakajijapan.github.io/blog/2013/10/22/how-to-combine-some-movies-with-avfoundation</id>
    <content type="html"><![CDATA[<p>こんにちはあのnakajijapanです。 以前、心霊動画アプリで「もう一度ご覧いただこう」 というアプリをリリースしました。 心霊動画アプリなので動画と動画を結合したり動画の上に動画を重ねて実装したりといろいろ やるわけですが今回はじめてということもあり沢山勉強になったのでここいらで自分の頭の整理 がてら情報をまためようと思います。</p>

<p>今回主に利用したのが「AV Foundation」です。</p>

<p>ざっくりいうとメディア情報（動画）を細かく制御できるようにしたフレームワークで、メタ情報の 取得、作成、編集、再エンコードができたりできます。</p>

<p>階層的には以下のような階層に存在して、簡単に動画とか写真の処理をしたい場合は</p>

<ul>
<li>Media Player Framework(MPMoviePlayerController, MPMoviePlayerViewController)</li>
<li>UIKit(UIIMagePickerController)</li>
</ul>


<p>を実装すれば難なく実装できちゃいます。ただ、今回は動画にいろんなエフェクトをいれたいのでもっと細かく制御 できる下の階層のフレームワークを使いました。感覚的に細かい制御できるようになるのでそのぶん面倒くさいのは いうまでもありません。</p>

<p><img src="/images/posts/2013-10-22_01.png" alt="layer" /></p>

<p><a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html">https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/Articles/00_Introduction.html</a></p>

<p>では実際にどう実装していけばいいかなのですが、そのために結構なクラスを利用するのでそれぞれざっくり説明していきます。</p>

<h3>AVAsset</h3>

<p>iPodや写真ライブラリのメディア情報をオブジェクトとして保持することができ、これからいろんなの情報を切り出して取得すること ができます。</p>

<p><code>obj-c
AVAsset*                   videoAsset;
videoAsset = [[AVURLAsset alloc] initWithURL:movieUrl options:nil];
</code></p>

<h3>AVAssetTrack</h3>

<p>アセットの情報からトラックレベルで切り出した情報。（うまく翻訳できなかった・・・） 例えば、アセットから動画と音声に切り分ける。</p>

<p>```obj-c
AVAsset<em>                   videoAsset;
AVAssetTrack</em>              videoTrack;
AVAssetTrack*              audioTrack;</p>

<p>videoAsset = [[AVURLAsset alloc] initWithURL:movieUrl options:nil];</p>

<p>// アセットからトラックを取得
videoTrack = [[videoAsset tracksWithMediaType:AVMediaTypeVideo] objectAtIndex:0];
audioTrack = [[videoAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];
```</p>

<h3>AVMutableCompositionTrack</h3>

<p>様々なメディア情報を結合したものです。このクラスで様々に編集された動画や音声を結合したり、時間の制御をしたり するクラスです。最終的にAVAssetExportSessionに渡してエキスポート処理（実際にファイルに保存する）します。</p>

<p>```obj-c
AVMutableComposition<em>      mixComposition;
AVMutableCompositionTrack</em> compositionVideoTrack;</p>

<p>// コンポジション作成
mixComposition = [AVMutableComposition composition];
compositionVideoTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];
```</p>

<h3>AVMutableVideoComposition</h3>

<p>AVMutableCompositionで新しいトラックを追加したときに返り値としてとれるもです。追加されたオブジェクトの参照で空のトラックって感じなんでしょうか。 ここに実際のメディア情報を入れていきます。メディアタイプで動画とか音声とか格納することができます。あと、この動画の何秒 から何秒間を何秒目に結合させるとかできたりします。</p>

<p>```obj-c
compositionVideoTrack = [mixComposition addMutableTrackWithMediaType:AVMediaTypeVideo preferredTrackID:kCMPersistentTrackID_Invalid];</p>

<p>[compositionVideoTrack insertTimeRange:CMTimeRangeMake(kCMTimeZero, timeDuration)</p>

<pre><code>                           ofTrack:_videoTrack
                            atTime:kCMTimeZero
                             error:nil];
</code></pre>

<p>[compositionVideoTrack setPreferredTransform:[videoTrack preferredTransform]];
```</p>

<h3>AVMutableVideoCompositionLayerInstruction</h3>

<p>アセットのトラックに対して回転、透過度、クロッピングができます。</p>

<p>```obj-c
// ここでは動画を小さくして指定の位置へ移動させてます。
CGAffineTransform scale      = CGAffineTransformMakeScale(0.30f, 0.30f);
CGAffineTransform trnsration = CGAffineTransformMakeTranslation(30, 406);</p>

<p>AVMutableVideoCompositionLayerInstruction* <em>layerInstruction;
</em>layerInstruction = [AVMutableVideoCompositionLayerInstruction videoCompositionLayerInstructionWithAssetTrack:<em>compositionVideoTrack];
[</em>layerInstruction setTransform:CGAffineTransformConcat(scale, trnsration) atTime:kCMTimeZero];
```</p>

<hr />

<h3>AVAssetExportSession</h3>

<p>さまざまなアセット情報を利用して指定されたフォーマットに変換したり動画のトリミングを行います。 例えば、mov形式、720x720サイズでファイルに保存させたりで</p>

<p>```obj-c
videoComp = [AVMutableVideoComposition videoComposition];
videoComp.renderSize    = CGSizeMake(720, 720);
videoComp.frameDuration = CMTimeMake(1, 24); // framerate</p>

<p>// AVCompositionをベースにAVAssetExportを生成
assetExportSession = [[AVAssetExportSession alloc] initWithAsset:mixComposition presetName:AVAssetExportPreset1280x720];</p>

<p>// 合成用のVideoCompositionを設定
assetExportSession.videoComposition = videoComp;</p>

<p>// エクスポートファイルの設定
NSURL *composedMovieUrl = [NSURL fileURLWithPath:composedMoviePath];
assetExportSession.outputFileType = AVFileTypeQuickTimeMovie;
assetExportSession.outputURL = composedMovieUrl;
assetExportSession.shouldOptimizeForNetworkUse = YES;</p>

<p>// エキスポート処理
[assetExportSession exportAsynchronouslyWithCompletionHandler:^{</p>

<pre><code>switch ([exportSession status]) {
    case AVAssetExportSessionStatusFailed:
        NSLog(@"Export failed: %@", [[exportSession error] localizedDescription]);
        break;
    case AVAssetExportSessionStatusCancelled:
        NSLog(@"Export canceled");
        break;
    default:
        break;
}
</code></pre>

<p>}];
```</p>

<p>主に利用するクラスを説明しました。これらを駆使して実装すれば簡単な動画の結合ができるようになります。</p>

<p>だいたいの大枠は以下の図を見ると何となくわかるかもしれませんね。</p>

<p><img src="/images/posts/2013-10-22_02.png" alt="layer" /></p>

<p><a href="https://developer.apple.com/library/ios/DOCUMENTATION/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html">https://developer.apple.com/library/ios/DOCUMENTATION/AudioVideo/Conceptual/AVFoundationPG/Articles/03_Editing.html</a></p>

<h2>まとめ</h2>

<p>どうでしょう。ざっくりとですが動画を結合するのに必要なクラスの説明とどのように実装されていくのかをざっくり 説明しました。本当に最初の方は？？？となってしまうと思いますが実装していくうちに分かってくるようになります。 あとこれ系の情報はあんまりネット上にはないのでしっかりと学びたいもっと動画カスタマイズしたという人がいれば やはり<code>AV Foundation Programming Guide</code>をじっくり読むのが最短だししっかり理解できるとおもいました。あと困った ことがあったら <a href="http://stackoverflow.com/">http://stackoverflow.com/</a> で同じ人が困っているかもしれないのでみるといいです。</p>

<p>応用編としては動画をスローモーションにさせたり、ワイプのような動画を作成したり、画像をアニメーションさせたりと いろいろありますが説明していこうと思います。というか時間がたったら忘れそうな知識なのでやります。。。</p>

<h2>Reference</h2>

<ul>
<li><a href="https://developer.apple.com/library/ios/documentation/AudioVideo/Conceptual/AVFoundationPG/AVFoundationPG.pdf">AV Foundation Programming Guide</a></li>
<li><a href="https://developer.apple.com/wwdc/videos/">Moving to AV Kit and AV Foundation – 606</a></li>
<li><a href="https://developer.apple.com/wwdc/videos/">Core Image Effects and Techniques – 509</a></li>
<li><a href="https://developer.apple.com/wwdc/videos/">Advanced Editing with AV Foundation – 612</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
